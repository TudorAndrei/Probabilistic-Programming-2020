{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pyro\n",
    "from pyro.optim import Adam, ClippedAdam\n",
    "from pyro.infer import SVI, Trace_ELBO, ELBO, TraceMeanField_ELBO\n",
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal, AutoGuide, AutoNormal\n",
    "from pyro.contrib.bnn import HiddenLayer\n",
    "from pyro.nn import PyroSample, PyroModule\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "from pyro.infer import Predictive\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.distributions import constraints\n",
    "\n",
    "from tqdm import trange, tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons, load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n",
    "pyro.set_rng_seed(42)\n",
    "# torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model,\n",
    "        loader,\n",
    "        epochs = 2,\n",
    "        multi=False,\n",
    "        optimizer=None,\n",
    "        criterion=nn.BCEWithLogitsLoss(),\n",
    "        lr=0.001):\n",
    "    \"\"\" Train the Neural network \"\"\"\n",
    "    epochs = trange(epochs)\n",
    "    if optimizer == None:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in epochs:\n",
    "        \n",
    "        running_loss = []\n",
    "        \n",
    "        for inputs, labels in loader:\n",
    "            \n",
    "            inputs = inputs.float()            \n",
    "            optimizer.zero_grad()            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            \n",
    "            if multi is False:\n",
    "                labels = labels.type_as(outputs)\n",
    "                labels = labels.unsqueeze(1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss.append(loss.item())\n",
    "            \n",
    "        loss = sum(running_loss) / len(running_loss)\n",
    "        string = f\"Loss = {loss:.5f}\"\n",
    "        epochs.set_postfix_str(s=string)\n",
    "\n",
    "\n",
    "def score(model, dataloader, type_ = 'test', multi=False):\n",
    "    \"\"\" Score the Neural Network\"\"\"\n",
    "    correct=0\n",
    "    total = 0\n",
    "    for test_inputs, test_labels in dataloader:\n",
    "\n",
    "        test_inputs = test_inputs.float()\n",
    "        outputs = model(test_inputs)\n",
    "        if multi:\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            acc = accuracy_score(np.array(predicted), np.array(test_labels))\n",
    "        else:\n",
    "            test_inputs = test_inputs.type_as(outputs)\n",
    "            predicted = torch.round(outputs).detach().numpy()\n",
    "            acc = accuracy_score(np.array(predicted), np.array(test_labels))\n",
    "    print(f\"Accuracy on {type_}: {100 * acc:.3f}%\") \n",
    "    \n",
    " ## Bayesian NN functions   \n",
    "def score_bnn(model, guide, dataloader, num_samples=800, type_='test'):\n",
    "    \"\"\" Score the Bayesian network using the Predictive class\"\"\"\n",
    "    predictive = Predictive(model, guide=guide, num_samples=num_samples,\n",
    "                    return_sites=(\"linear.weight\", \"obs\", \"_RETURN\", \"data\"))\n",
    "    for data, labels in dataloader:\n",
    "        pred = predictive(data)\n",
    "        pred_vals = pred[\"_RETURN\"].detach().numpy().mean(axis=0).round()\n",
    "        lables = labels.unsqueeze(1)\n",
    "        acc = accuracy_score(lables, pred_vals)\n",
    "    \n",
    "    print(f\"Accuracy on {type_}: {100 * acc}%\")\n",
    "    \n",
    "\n",
    "def predict(data,labels,  guide):\n",
    "    num_samples = 10\n",
    "    sampled_models = [guide(None, None) for _ in range(num_samples)]\n",
    "    predictions = [m(data).data for m in sampled_models]\n",
    "    mean = torch.mean(torch.stack(predictions), 0)\n",
    "    return np.argmax(mean.numpy(), axis=1)\n",
    "\n",
    "\n",
    "def score_bnn2(guide, dataloader, type_ = 'test'):\n",
    "    \"\"\" Score the Bayesian Network using the guide to predict\"\"\"\n",
    "    for  data, labels in dataloader:\n",
    "        predicted = predict(data.float(),labels.float(), guide)\n",
    "        acc = accuracy_score(np.array(predicted),np.array(labels))\n",
    "#         correct += (np.array(predicted) == np.array(labels)).sum().item()\n",
    "\n",
    "    print(f\"Accuracy on {type_}: {100 * acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sanity = np.random.randn(200,2)\n",
    "\n",
    "class SanityDataset(torch.utils.data.dataset.Dataset):\n",
    "    def __init__(self, X):\n",
    "\n",
    "        self.X = X\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        y = np.tanh(self.X[idx, 0] + self.X[idx, 1])\n",
    "        y =  1. / ( 1. + np.exp(-( y + y)))\n",
    "        y = (y > 0.5).astype(np.int)\n",
    "        return (self.X[idx,:]).astype(np.float32), y\n",
    "    \n",
    "split = 100\n",
    "train_sanity = torch.utils.data.DataLoader(SanityDataset(X_sanity[:split]), batch_size=split, shuffle=True)\n",
    "test_sanity = torch.utils.data.DataLoader(SanityDataset(X_sanity[split:]), batch_size=split,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SanityModel(nn.Module):\n",
    "    def __init__(self, num_in=2):\n",
    "        super(SanityModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_in, 2)\n",
    "        self.fc2 = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 246.56it/s, Loss = 0.24957]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test: 99.000%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sanity = SanityModel()\n",
    "fit(sanity, train_sanity, 100, criterion=nn.BCELoss(), lr=0.01)\n",
    "score(sanity, test_sanity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianSanityCheck(PyroModule):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear = PyroModule[nn.Linear](in_features, out_features)\n",
    "        self.linear.weight = PyroSample(dist.Normal(0., 1.).expand([out_features, in_features]).to_event(2))\n",
    "        self.linear.bias = PyroSample(dist.Normal(0., 1.).expand([out_features]).to_event(1))\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        param = torch.sigmoid(self.linear(x))\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            obs = pyro.sample(\"obs\", dist.Bernoulli(param), obs=y)\n",
    "        return param\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace_ELBO, ELBO, TraceMeanField_ELBO\n",
    "bnn = BayesianSanityCheck(2,1)\n",
    "guide = AutoNormal(bnn)\n",
    "adam = pyro.optim.Adam({\"lr\": 0.01})\n",
    "svi = SVI(bnn, guide, adam, loss=TraceMeanField_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 119.66it/s, Loss = 68.53079]\n"
     ]
    }
   ],
   "source": [
    "epochs = trange(20)\n",
    "bnn.train()\n",
    "for i in epochs:\n",
    "    for data, labels in train_sanity:\n",
    "        bnn.zero_grad()\n",
    "        labels = labels.float()\n",
    "        loss = svi.step(data, labels)\n",
    "        loss = loss / len(labels)\n",
    "\n",
    "\n",
    "    string = f\"Loss = {loss:.5f}\"\n",
    "    epochs.set_postfix_str(s=string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test: 48.0%\n"
     ]
    }
   ],
   "source": [
    "score_bnn(bnn, guide, test_sanity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Moons dataset\n",
    "\n",
    "- uses self created guide, instead of AutoGuide\n",
    "- uses poutine for prediction, instead of the PredictClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_moon, y_moon = make_moons(n_samples=150, noise=.05)\n",
    "\n",
    "class MoonDataset(torch.utils.data.dataset.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "    \n",
    "        return self.X[idx,:], self.y[idx]\n",
    "\n",
    "moon_split = 100\n",
    "train_moon = DataLoader(MoonDataset(X_moon[:moon_split], y_moon[:moon_split]),batch_size=moon_split, shuffle=True)\n",
    "test_moon = DataLoader(MoonDataset(X_moon[moon_split:], y_moon[moon_split:]),batch_size=moon_split, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoonModel(nn.Module):\n",
    "    def __init__(self, num_in=2):\n",
    "        super(MoonModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_in, 4)\n",
    "        self.fc2 = nn.Linear(4, 10)\n",
    "        \n",
    "        self.out = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        X = torch.relu(x)\n",
    "        \n",
    "        x = self.out(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "moon_model = MoonModel(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 352.73it/s, Loss = 0.64794]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test: 84.000%\n"
     ]
    }
   ],
   "source": [
    "fit(moon_model, train_moon, 100, criterion=nn.BCELoss(), lr=0.001)\n",
    "score(moon_model, test_moon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BNN, self).__init__()\n",
    "        self.n_indims = 2\n",
    "        self.n_hidden = 10\n",
    "        self.n_classes = 1\n",
    "       \n",
    "    def model(self, data, labels=None):\n",
    "        \n",
    "        n_data = len(data)\n",
    "        if labels is not None:\n",
    "            labels = labels.float()\n",
    "            \n",
    "        # Create the tensors for the normal distributions\n",
    "        # The Hidden Layer uses Normal distribution\n",
    "        \n",
    "        l1_mean = torch.zeros(self.n_indims, self.n_hidden)\n",
    "        l1_scale = torch.ones(self.n_indims, self.n_hidden) \n",
    "        \n",
    "        l2_mean = torch.zeros(self.n_hidden + 1, self.n_classes)\n",
    "        l2_scale = torch.ones(self.n_hidden + 1, self.n_classes)\n",
    "        \n",
    "        \n",
    "        with pyro.plate('data', size=n_data):\n",
    "            # Connect the layers\n",
    "            h1 = pyro.sample('h1', HiddenLayer(data, l1_mean,l1_scale, \n",
    "                                                   non_linearity=F.relu,))\n",
    "           \n",
    "            logits = pyro.sample('logits', HiddenLayer(h1, l2_mean, l2_scale,\n",
    "                                                           non_linearity=F.sigmoid,\n",
    "                                                           include_hidden_bias=False))\n",
    "            \n",
    "            \n",
    "            return pyro.sample('label', dist.Bernoulli(logits=logits), obs=labels) \n",
    "    \n",
    "    def guide(self, data, labels=None):\n",
    "        n_data = len(data)\n",
    "        if labels is not None:\n",
    "            labels = labels.float()\n",
    "\n",
    "        l1_mean = pyro.param('l1_mean', 0.1 * torch.randn(self.n_indims, self.n_hidden))\n",
    "        l1_scale = pyro.param('l1_scale', 0.1 * torch.ones(self.n_indims, self.n_hidden),\n",
    "                              constraint=constraints.greater_than(0.01))\n",
    "        \n",
    "        l2_mean = pyro.param('l2_mean', 0.1 * torch.randn(self.n_hidden + 1, self.n_classes))\n",
    "        l2_scale = pyro.param('l2_scale', 0.1 * torch.ones(self.n_hidden + 1, self.n_classes),\n",
    "                              constraint=constraints.greater_than(0.01))\n",
    "        \n",
    "        with pyro.plate('data', size=n_data):\n",
    "            h1 = pyro.sample('h1', HiddenLayer(data,\n",
    "                                               l1_mean,\n",
    "                                               l1_scale, \n",
    "                                               non_linearity=torch.tanh))\n",
    "            \n",
    "            logits = pyro.sample('logits', HiddenLayer(h1,\n",
    "                                                       l2_mean,\n",
    "                                                       l2_scale,\n",
    "                                                       non_linearity=torch.sigmoid,\n",
    "                                                       include_hidden_bias=False))\n",
    "    \n",
    "    \n",
    "    def infer(self, loader, lr=0.01, momentum=0.9,\n",
    "                         num_epochs=30):\n",
    "        optim = ClippedAdam({'lr': lr})\n",
    "        elbo =  TraceMeanField_ELBO()\n",
    "        svi = SVI(self.model, self.guide, optim, elbo)\n",
    "        epochs = trange(num_epochs)\n",
    "        for i in epochs:\n",
    "            for data, labels in loader:\n",
    "                loss = svi.step(data.float(), labels)  / len(labels)                \n",
    "            string = f\"Loss = {loss:.5f}\"\n",
    "            epochs.set_postfix_str(s=string)\n",
    "\n",
    "    def forward(self, images, n_samples=10):\n",
    "        res = []\n",
    "        for i in range(n_samples):\n",
    "            t = poutine.trace(self.guide).get_trace(images)\n",
    "            res.append(t.nodes['logits']['value'])\n",
    "        return torch.stack(res, dim=0) \n",
    "\n",
    "    def score(self, dataloader, type_ = 'test'):\n",
    "        \"\"\" Score the BNN using the poutine library\"\"\"\n",
    "        for data, labels in dataloader:            \n",
    "            predicted = bayesnn.forward(data.float(), 10)\n",
    "            predicted = predicted.detach().numpy().mean(axis=0).round().squeeze()\n",
    "            acc = accuracy_score(np.array(predicted), np.array(labels))\n",
    "\n",
    "        print(f\"Accuracy on {type_}: {acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 166.05it/s, Loss = 70.16169]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test: 48.00%\n"
     ]
    }
   ],
   "source": [
    "pyro.clear_param_store()\n",
    "bayesnn = BNN()\n",
    "bayesnn.infer(train_moon, num_epochs=100, lr=0.001)\n",
    "bayesnn.score(test_moon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi class classification using Transfer learning\n",
    "\n",
    "- The guide fuction samples from the weights of the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wine, y_wine = load_wine(return_X_y=True)\n",
    "size=100\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_wine, y_wine, train_size=size, random_state=42)\n",
    "\n",
    "class WineDataset(torch.utils.data.dataset.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X, self.y = X, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index,:], self.y[index]\n",
    "    \n",
    "train_wine = DataLoader(WineDataset(X_train, y_train),batch_size=100, shuffle=True)\n",
    "test_wine = DataLoader(WineDataset(X_test,y_test),batch_size=len(X_wine) - 100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WineModel(nn.Module):\n",
    "    def __init__(self, num_in, n_classes):\n",
    "\n",
    "        super(WineModel, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(num_in, 16)\n",
    "        self.fc2 = nn.Linear(16, 4)\n",
    "        self.fc3 = nn.Linear(4, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 328.05it/s, Loss = 0.70001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test: 67.949%\n"
     ]
    }
   ],
   "source": [
    "wine_model = WineModel(13, 3)\n",
    "fit(wine_model, train_wine, 200, criterion=torch.nn.CrossEntropyLoss(), multi=True, lr=0.01)\n",
    "score(wine_model, test_wine, multi=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_wine(data, labels):\n",
    "    fc1w_prior = dist.Normal(loc=torch.zeros_like(wine_model.fc1.weight),\n",
    "                             scale=torch.ones_like(wine_model.fc1.weight))\n",
    "    fc1b_prior = dist.Normal(loc=torch.zeros_like(wine_model.fc1.bias),\n",
    "                             scale=torch.ones_like(wine_model.fc1.bias))\n",
    "\n",
    "    fc2w_prior = dist.Normal(loc=torch.zeros_like(wine_model.fc2.weight),\n",
    "                             scale=torch.ones_like(wine_model.fc2.weight))\n",
    "    fc2b_prior = dist.Normal(loc=torch.zeros_like(wine_model.fc2.bias),\n",
    "                             scale=torch.ones_like(wine_model.fc2.bias))\n",
    "\n",
    "\n",
    "    fc3w_prior = dist.Normal(loc=torch.zeros_like(wine_model.fc3.weight),\n",
    "                             scale=torch.ones_like(wine_model.fc3.weight))\n",
    "    fc3b_prior = dist.Normal(loc=torch.zeros_like(wine_model.fc3.bias),\n",
    "                             scale=torch.ones_like(wine_model.fc3.bias))\n",
    "\n",
    "    priors = {\"fc1w\": fc1w_prior,\n",
    "              \"fc1b\": fc1b_prior,\n",
    "              \"fc2w\": fc2w_prior,\n",
    "              \"fc2b\": fc2b_prior,\n",
    "              \"fc3w\": fc3w_prior,\n",
    "              \"fc3b\": fc3b_prior}\n",
    "\n",
    "    lifted_module = pyro.random_module(\"module\", wine_model, priors)\n",
    "    lifted_reg_model = lifted_module()\n",
    "\n",
    "    probs = torch.nn.functional.log_softmax(lifted_reg_model(data),dim=1)\n",
    "\n",
    "    pyro.sample(\"obs\", dist.Categorical(logits=probs), obs=labels)\n",
    "    \n",
    "def guide_wine(data, labels):\n",
    "    \n",
    "    # FC1 weights\n",
    "    fc1w_mu = torch.randn_like(wine_model.fc1.weight)\n",
    "    fc1w_sigma = torch.randn_like(wine_model.fc1.weight)\n",
    "    fc1w_mu_param = pyro.param(\"fc1w_mu\", fc1w_mu)\n",
    "    fc1w_sigma_param = F.softplus(pyro.param(\"fc1w_sigma\", fc1w_sigma))\n",
    "    fc1w_approx_post = dist.Normal(loc=fc1w_mu_param, scale=fc1w_sigma_param)\n",
    "    # FC1 bias\n",
    "    fc1b_mu = torch.randn_like(wine_model.fc1.bias)\n",
    "    fc1b_sigma = torch.randn_like(wine_model.fc1.bias)\n",
    "    fc1b_mu_param = pyro.param(\"fc1b_mu\", fc1b_mu)\n",
    "    fc1b_sigma_param = F.softplus(pyro.param(\"fc1b_sigma\", fc1b_sigma))\n",
    "    fc1b_approx_post = dist.Normal(loc=fc1b_mu_param, scale=fc1b_sigma_param)\n",
    "    \n",
    "    # FC2 weights\n",
    "    fc2w_mu = torch.randn_like(wine_model.fc2.weight)\n",
    "    fc2w_sigma = torch.randn_like(wine_model.fc2.weight)\n",
    "    fc2w_mu_param = pyro.param(\"fc2w_mu\", fc2w_mu)\n",
    "    fc2w_sigma_param = F.softplus(pyro.param(\"fc2w_sigma\", fc2w_sigma))\n",
    "    fc2w_approx_post = dist.Normal(loc=fc2w_mu_param, scale=fc2w_sigma_param)\n",
    "    # FC2 bias\n",
    "    fc2b_mu = torch.randn_like(wine_model.fc2.bias)\n",
    "    fc2b_sigma = torch.randn_like(wine_model.fc2.bias)\n",
    "    fc2b_mu_param = pyro.param(\"fc2b_mu\", fc2b_mu)\n",
    "    fc2b_sigma_param = F.softplus(pyro.param(\"fc2b_sigma\", fc2b_sigma))\n",
    "    fc2b_approx_post = dist.Normal(loc=fc2b_mu_param, scale=fc2b_sigma_param)\n",
    "        \n",
    "    # FC3 weights\n",
    "    fc3w_mu = torch.randn_like(wine_model.fc3.weight)\n",
    "    fc3w_sigma = torch.randn_like(wine_model.fc3.weight)\n",
    "    fc3w_mu_param = pyro.param(\"fc3w_mu\", fc3w_mu)\n",
    "    fc3w_sigma_param = F.softplus(pyro.param(\"fc3w_sigma\", fc3w_sigma))\n",
    "    fc3w_approx_post = dist.Normal(loc=fc3w_mu_param, scale=fc3w_sigma_param).independent(1)\n",
    "    # FC2 bias\n",
    "    fc3b_mu = torch.randn_like(wine_model.fc3.bias)\n",
    "    fc3b_sigma = torch.randn_like(wine_model.fc3.bias)\n",
    "    fc3b_mu_param = pyro.param(\"fc3b_mu\", fc3b_mu)\n",
    "    fc3b_sigma_param = F.softplus(pyro.param(\"fc3b_sigma\", fc3b_sigma))\n",
    "    fc3b_approx_post = dist.Normal(loc=fc3b_mu_param, scale=fc3b_sigma_param)\n",
    "    \n",
    "    posterior = {\"fc1w\": fc1w_approx_post,\n",
    "                \"fc1b\": fc1b_approx_post,\n",
    "                \"fc2w\": fc2w_approx_post,\n",
    "                \"fc2b\": fc2b_approx_post,\n",
    "                \"fc3w\": fc3w_approx_post,\n",
    "                \"fc3b\": fc3b_approx_post,}\n",
    "\n",
    "    lifted_module = pyro.random_module(\"module\", wine_model, posterior)\n",
    "    \n",
    "    return lifted_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 119.52it/s, Loss = 68.10476]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test: 70.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optim = pyro.optim.Adam({\"lr\": 0.001})\n",
    "svi_wine = SVI(model_wine, guide_wine, optim, loss=Trace_ELBO())\n",
    "# train the model\n",
    "epochs = trange(10)\n",
    "for i in epochs:\n",
    "    loss = 0\n",
    "    for data, labels in train_wine:\n",
    "        loss = svi_wine.step(data.float(), labels)\n",
    "\n",
    "    string = f\"Loss = {loss:.5f}\"\n",
    "    epochs.set_postfix_str(s=string)\n",
    "    \n",
    "score_bnn2(guide_wine, test_wine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- [Pyro Docs](https://docs.pyro.ai/en/stable/index.html)\n",
    "- [Pyro Examples](https://pyro.ai/examples/bayesian_regression.html#)\n",
    "- [Pyro MNIST](https://alsibahi.xyz/snippets/2019/06/15/pyro_mnist_bnn_kl.html)\n",
    "- [Making Your Neural Network Say “I Don’t Know” — Bayesian NNs using Pyro and PyTorch](https://towardsdatascience.com/making-your-neural-network-say-i-dont-know-bayesian-nns-using-pyro-and-pytorch-b1c24e6ab8cd)\n",
    "- [Bayesian Neural Networks: 1 Why Bother?](https://towardsdatascience.com/bayesian-neural-networks-1-why-bother-b585375b38ec)\n",
    "- [Bayesian Neural Networks: 2 Fully Connected in TensorFlow and Pytorch](https://towardsdatascience.com/bayesian-neural-networks-2-fully-connected-in-tensorflow-and-pytorch-7bf65fb4697)\n",
    "- [Bayesian Neural Networks: 3 Bayesian CNN](https://towardsdatascience.com/bayesian-neural-networks-3-bayesian-cnn-6ecd842eeff3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pp] *",
   "language": "python",
   "name": "conda-env-pp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
