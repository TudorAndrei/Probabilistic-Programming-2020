{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "million-characterization",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pyro\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer import SVI, Trace_ELBO, ELBO\n",
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal, AutoGuide\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import trange, tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons, load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "pyro.set_rng_seed(42)\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "timely-connection",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model,\n",
    "        loader,\n",
    "        epochs = 2,\n",
    "        optimizer=None,\n",
    "        criterion=nn.BCEWithLogitsLoss()):\n",
    "    \n",
    "    learning_rate = 0.001\n",
    "    epochs = trange(epochs)\n",
    "\n",
    "    if optimizer == None:\n",
    "        optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for epoch in epochs:\n",
    "        \n",
    "        running_loss = []\n",
    "        for i, batch in enumerate(loader):\n",
    "            inputs, labels = batch['X'], batch['y']\n",
    "            \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss.append(loss.item())\n",
    "\n",
    "            freq_checks = 5\n",
    "        loss = sum(running_loss) / len(running_loss)\n",
    "        string = f\"Loss = {loss:.5f}\"\n",
    "        epochs.set_postfix_str(s=string)\n",
    "\n",
    "    print(f\"Final loss = {loss:.5f}\")\n",
    "    \n",
    "def score(model, dataloader, type_ = 'test'):\n",
    "    for data in dataloader:\n",
    "        test_inputs, test_labels = data['X'], data['y']\n",
    "\n",
    "#         print(data)\n",
    "        outputs = model(test_inputs)\n",
    "#         print()\n",
    "#         print(outputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "#         print(predicted)\n",
    "#         print(test_labels)\n",
    "    accuracy = sum(np.array(predicted) == np.array(test_labels))/len(predicted)\n",
    "    print(f\"Accuracy on {type_}: {accuracy*100}%\")\n",
    "    \n",
    "def predict(data, guide):\n",
    "    num_samples = 10\n",
    "    sampled_models = [guide(None) for _ in range(num_samples)]\n",
    "    predictions = [m(data).data for m in sampled_models]\n",
    "    mean = torch.mean(torch.stack(predictions), 0)\n",
    "    return np.argmax(mean.numpy(), axis=1)\n",
    "\n",
    "def score_bnn(guide, dataloader, type_ = 'test'):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for  data in dataloader:\n",
    "        test_inputs, test_labels = data['X'], data['y']\n",
    "\n",
    "        predicted = predict(test_inputs, guide)\n",
    "        total += test_labels.size(0)\n",
    "        correct += (np.array(predicted) == np.array(test_labels)).sum().item()\n",
    "\n",
    "    print(f\"Accuracy on {type_}: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "arctic-petroleum",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wine, y_wine = load_wine(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_wine, y_wine, train_size=100, random_state=42)\n",
    "\n",
    "class WineDataset(torch.utils.data.dataset.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X, self.y = X, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\"X\": self.X[index,:], \"y\": self.y[index]}\n",
    "    \n",
    "train_wine = DataLoader(WineDataset(X_train, y_train), shuffle=True)\n",
    "test_wine = DataLoader(WineDataset(X_test,y_test), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "authorized-reviewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WineModel(nn.Module):\n",
    "    def __init__(self, num_in, n_classes):\n",
    "\n",
    "        super(WineModel, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(num_in, 10)\n",
    "        self.fc2 = nn.Linear(10, 5)\n",
    "        self.fc3 = nn.Linear(5, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "introductory-estonia",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  9.98it/s, Loss = 0.84979]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss = 0.84979\n",
      "Accuracy on train: 100.0%\n",
      "Accuracy on test: 100.0%\n"
     ]
    }
   ],
   "source": [
    "wine_model = WineModel(13, 3)\n",
    "fit(wine_model, train_wine, 10, criterion=torch.nn.CrossEntropyLoss())\n",
    "score(wine_model, train_wine, 'train')\n",
    "score(wine_model, test_wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "behavioral-ghost",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_wine(data):\n",
    "    fc1w_prior = dist.Normal(loc=torch.zeros_like(wine_model.fc1.weight),\n",
    "                             scale=torch.ones_like(wine_model.fc1.weight))\n",
    "    fc1b_prior = dist.Normal(loc=torch.zeros_like(wine_model.fc1.bias),\n",
    "                             scale=torch.ones_like(wine_model.fc1.bias))\n",
    "\n",
    "    fc2w_prior = dist.Normal(loc=torch.zeros_like(wine_model.fc2.weight),\n",
    "                             scale=torch.ones_like(wine_model.fc2.weight))\n",
    "    fc2b_prior = dist.Normal(loc=torch.zeros_like(wine_model.fc2.bias),\n",
    "                             scale=torch.ones_like(wine_model.fc2.bias))\n",
    "\n",
    "\n",
    "    fc3w_prior = dist.Normal(loc=torch.zeros_like(wine_model.fc3.weight),\n",
    "                             scale=torch.ones_like(wine_model.fc3.weight))\n",
    "    fc3b_prior = dist.Normal(loc=torch.zeros_like(wine_model.fc3.bias),\n",
    "                             scale=torch.ones_like(wine_model.fc3.bias))\n",
    "\n",
    "    priors = {\"fc1w\": fc1w_prior,\n",
    "              \"fc1b\": fc1b_prior,\n",
    "              \"fc2w\": fc2w_prior,\n",
    "              \"fc2b\": fc2b_prior,\n",
    "              \"fc3w\": fc3w_prior,\n",
    "              \"fc3b\": fc3b_prior}\n",
    "\n",
    "    lifted_module = pyro.random_module(\"module\", wine_model, priors)\n",
    "    lifted_reg_model = lifted_module()\n",
    "\n",
    "    probs = torch.nn.functional.log_softmax(lifted_reg_model(data[\"X\"]),dim=1)\n",
    "\n",
    "    pyro.sample(\"obs\", dist.Categorical(logits=probs), obs=data[\"y\"])\n",
    "    \n",
    "def guide_wine(data):\n",
    "    \n",
    "    # FC1 weights\n",
    "    fc1w_mu = torch.randn_like(wine_model.fc1.weight)\n",
    "    fc1w_sigma = torch.randn_like(wine_model.fc1.weight)\n",
    "    fc1w_mu_param = pyro.param(\"fc1w_mu\", fc1w_mu)\n",
    "    fc1w_sigma_param = F.softplus(pyro.param(\"fc1w_sigma\", fc1w_sigma))\n",
    "    fc1w_approx_post = dist.Normal(loc=fc1w_mu_param, scale=fc1w_sigma_param)\n",
    "    # FC1 bias\n",
    "    fc1b_mu = torch.randn_like(wine_model.fc1.bias)\n",
    "    fc1b_sigma = torch.randn_like(wine_model.fc1.bias)\n",
    "    fc1b_mu_param = pyro.param(\"fc1b_mu\", fc1b_mu)\n",
    "    fc1b_sigma_param = F.softplus(pyro.param(\"fc1b_sigma\", fc1b_sigma))\n",
    "    fc1b_approx_post = dist.Normal(loc=fc1b_mu_param, scale=fc1b_sigma_param)\n",
    "    \n",
    "    # FC2 weights\n",
    "    fc2w_mu = torch.randn_like(wine_model.fc2.weight)\n",
    "    fc2w_sigma = torch.randn_like(wine_model.fc2.weight)\n",
    "    fc2w_mu_param = pyro.param(\"fc2w_mu\", fc2w_mu)\n",
    "    fc2w_sigma_param = F.softplus(pyro.param(\"fc2w_sigma\", fc2w_sigma))\n",
    "    fc2w_approx_post = dist.Normal(loc=fc2w_mu_param, scale=fc2w_sigma_param)\n",
    "    # FC2 bias\n",
    "    fc2b_mu = torch.randn_like(wine_model.fc2.bias)\n",
    "    fc2b_sigma = torch.randn_like(wine_model.fc2.bias)\n",
    "    fc2b_mu_param = pyro.param(\"fc2b_mu\", fc2b_mu)\n",
    "    fc2b_sigma_param = F.softplus(pyro.param(\"fc2b_sigma\", fc2b_sigma))\n",
    "    fc2b_approx_post = dist.Normal(loc=fc2b_mu_param, scale=fc2b_sigma_param)\n",
    "        \n",
    "    # FC3 weights\n",
    "    fc3w_mu = torch.randn_like(wine_model.fc3.weight)\n",
    "    fc3w_sigma = torch.randn_like(wine_model.fc3.weight)\n",
    "    fc3w_mu_param = pyro.param(\"fc3w_mu\", fc3w_mu)\n",
    "    fc3w_sigma_param = F.softplus(pyro.param(\"fc3w_sigma\", fc3w_sigma))\n",
    "    fc3w_approx_post = dist.Normal(loc=fc3w_mu_param, scale=fc3w_sigma_param).independent(1)\n",
    "    # FC2 bias\n",
    "    fc3b_mu = torch.randn_like(wine_model.fc3.bias)\n",
    "    fc3b_sigma = torch.randn_like(wine_model.fc3.bias)\n",
    "    fc3b_mu_param = pyro.param(\"fc3b_mu\", fc3b_mu)\n",
    "    fc3b_sigma_param = F.softplus(pyro.param(\"fc3b_sigma\", fc3b_sigma))\n",
    "    fc3b_approx_post = dist.Normal(loc=fc3b_mu_param, scale=fc3b_sigma_param)\n",
    "    \n",
    "    posterior = {\"fc1w\": fc1w_approx_post,\n",
    "                        \"fc1b\": fc1b_approx_post,\n",
    "                        \"fc2w\": fc2w_approx_post,\n",
    "                        \"fc2b\": fc2b_approx_post,\n",
    "                        \"fc3w\": fc3w_approx_post,\n",
    "                        \"fc3b\": fc3b_approx_post,}\n",
    "\n",
    "    lifted_module = pyro.random_module(\"module\", wine_model, posterior)\n",
    "    \n",
    "    return lifted_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "coordinated-father",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.81it/s, Loss = 1.08650]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train: 42.00%\n",
      "Accuracy on test: 37.18%\n"
     ]
    }
   ],
   "source": [
    "optim = pyro.optim.Adam({\"lr\": 0.001})\n",
    "svi_wine = SVI(model_wine, guide_wine, optim, loss=Trace_ELBO())\n",
    "loss = 0\n",
    "epochs = trange(10)\n",
    "total_epoch_loss_train = 0\n",
    "for i in epochs:\n",
    "    loss = 0\n",
    "    for batch_id, data in enumerate(train_wine):\n",
    "        # calculate the loss and take a gradient step\n",
    "        loss += svi_wine.step(data)\n",
    "    total_epoch_loss_train = loss / len(train_wine.dataset)\n",
    "\n",
    "    string = f\"Loss = {total_epoch_loss_train:.5f}\"\n",
    "    epochs.set_postfix_str(s=string)\n",
    "    \n",
    "score_bnn(guide_wine, train_wine, 'train')\n",
    "score_bnn(guide_wine, test_wine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
