{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/tudor/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymc as pm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pymc.CommonDeterministics import CompletedDirichlet as CompDir\n",
    "from pymc import Dirichlet as Dir\n",
    "from pymc import Categorical as Cat\n",
    "from pymc import Model, MCMC\n",
    "import math\n",
    "## This is to create a small dataset from the bbc new dataset\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "np.random.seed(42)\n",
    "from collections import Counter\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA(object):\n",
    "    def __init__(self, data, nr_of_topics, a =1, b=1, iterations=4000):\n",
    "\n",
    "        self.K = nr_of_topics\n",
    "        self.M = len(data) # nr_of_documents\n",
    "#         print(self.M)\n",
    "        self.N = [len(document) for document in data] # nr of words for each document\n",
    "        self.iter = iterations\n",
    "        self.burnin = iterations/4\n",
    "        \n",
    "        # create a list of unique words\n",
    "        words = []\n",
    "        for document in data:\n",
    "            for word in document:\n",
    "                if word not in words:\n",
    "                    words.append(word)\n",
    "        \n",
    "        \n",
    "        self.V = len(words)\n",
    "        self.encoder = LabelEncoder()\n",
    "        \n",
    "        # a,b are used to assign different values to alpha and beta\n",
    "        self.alpha = np.ones(self.K) * a\n",
    "        self.beta = np.ones(self.V) * b\n",
    "        \n",
    "        self.vocabulary = words\n",
    "        self.documents =  self.preprocess(data, words)\n",
    "\n",
    "    def compileModel(self):\n",
    "    \n",
    "        # Create the probabilistic variables\n",
    "        self.prior_phi = pm.Container([Dir(f'prior_phi_{k}', self.beta) for k in range(self.K)])\n",
    "        self.phi = pm.Container([CompDir(f'phi_{k}', self.prior_phi[k]) for k in range(self.K)])\n",
    "\n",
    "        self.prior_theta = pm.Container([Dir(f'prior_theta_{m}', self.alpha) for m in range(self.M)])\n",
    "        self.theta = pm.Container([CompDir(f'theta_{m}', self.prior_theta[m]) for m in range(self.M)])\n",
    "\n",
    "        self.z = pm.Container([Cat(f\"z_{m}\",\n",
    "                                p=self.theta[m],\n",
    "                                size=self.N[m],\n",
    "                                value=np.random.randint(self.K, size=self.N[m])) for m in range(self.M)])\n",
    "\n",
    "        self.w = pm.Container([Cat(f\"w_{m}_{n}\",\n",
    "                                p=pm.Lambda(f\"phi_{m}_{n}\", lambda z=self.z[m][n],phi=self.phi:phi[z]),\n",
    "                                value=self.documents[m][n],\n",
    "                                observed=True,\n",
    "                                verbose=False) for m in range(self.M) for n in range(self.N[m])])\n",
    "\n",
    "        # create the model\n",
    "        self.model = pm.Model([self.prior_phi, self.prior_theta, self.phi, self.theta, self.z, self.w])\n",
    "        self.mcmc = pm.MCMC(self.model)\n",
    "        self.mcmc.sample(self.iter,self.burnin, thin=1)\n",
    "     \n",
    "\n",
    "    def preprocess(self, data, words):\n",
    "        # encode the data\n",
    "        self.encoder = LabelEncoder().fit(words)\n",
    "        documents = []\n",
    "        for document in data:\n",
    "                documents.append(self.encoder.transform(document).tolist())\n",
    "        return documents\n",
    "    \n",
    "    def __del__(self):\n",
    "        pass\n",
    "\n",
    "    def showResults(self):\n",
    "        # print the results of theta, phi and z\n",
    "        theta_values = [self.mcmc.trace(f\"theta_{m}\")[:].mean(axis=0) for m in range(self.M)]\n",
    "        phi_values = [self.mcmc.trace(f\"phi_{k}\")[:].mean(axis=0) for k in range(self.K)]\n",
    "        z_values = [np.round(self.mcmc.trace(f\"z_{m}\")[:].mean(axis=0)) for m in range(self.M)]\n",
    "        \n",
    "        print(f\"Theta is\\n\")\n",
    "        for thetas in theta_values:\n",
    "            print(thetas)\n",
    "        print(f\"Phi is \\n\")\n",
    "        for phis in phi_values:\n",
    "            print(phis)\n",
    "        print(f\"Z is \\n\")\n",
    "        for zs in z_values:\n",
    "            print(zs)\n",
    "\n",
    "    \n",
    "    def mostImportantNWords(self, n=5):\n",
    "        # print the most important words\n",
    "        phi_values = [self.mcmc.trace(f\"phi_{k}\")[:].mean(axis=0) for k in range(self.K)]\n",
    "        for i, values in enumerate(phi_values):\n",
    "            print(f\"Topic {i}\")\n",
    "            ids  = np.argsort(phi_values[i][0])[::-1]\n",
    "            ids = ids[:n]\n",
    "            restult = \"\"\n",
    "            for id_ in ids:\n",
    "                restult +=   str(self.vocabulary[id_]) + \", \"\n",
    "            print(restult)\n",
    "            \n",
    "    ## Task 2\n",
    "    def documentSimilarity(self, threshold):\n",
    "        # compare paris of documents and compute the hellinger distance\n",
    "        theta_values = [self.mcmc.trace(f\"theta_{m}\")[:].mean(axis=0) for m in range(self.M)]\n",
    "        similarities = []\n",
    "        for index_doc1, theta1 in enumerate(theta_values):\n",
    "            for index_doc2, theta2 in enumerate(theta_values):\n",
    "                if index_doc1 != index_doc2:\n",
    "                    similarities.append([index_doc1, index_doc2, 1 - self.hellingerDistance(theta1, theta2)])\n",
    "\n",
    "        similarities = [similarity for similarity in similarities if similarity[2] > threshold]\n",
    "        \n",
    "        return similarities\n",
    "\n",
    "    @staticmethod\n",
    "    def hellingerDistance(document1, document2):\n",
    "        score = 0\n",
    "        for index in range(len(document1[0])):\n",
    "            score = score + pow((math.sqrt(document1[0][index]) - (math.sqrt(document2[0][index]))), 2)\n",
    "\n",
    "        return round(score,4)\n",
    "\n",
    "                \n",
    "    def assignNewTopic(self, document):\n",
    "        # assign to a new document a topic\n",
    "        seen_words = [x for x in document if x in self.vocabulary]\n",
    "        encoded = self.encoder.transform(seen_words)\n",
    "        phi_values = [self.mcmc.trace(f\"phi_{k}\")[:].mean(axis=0) for k in range(self.K)]\n",
    "        probs = []\n",
    "        for topic in range(self.K):\n",
    "            topic_prob = 0\n",
    "            for index in encoded:\n",
    "                topic_prob += phi_values[topic][0][index]\n",
    "            probs.append(topic_prob)\n",
    "        \n",
    "        max_index = np.argsort(probs)[-1]\n",
    "        print(\"The topic of the document is \",max_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tudor/.miniconda3/envs/pp/lib/python3.6/site-packages/pymc/MCMC.py:81: UserWarning: Instantiating a Model object directly is deprecated. We recommend passing variables directly to the Model subclass.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-----------------100%-----------------] 1000 of 1000 complete in 1.3 sec"
     ]
    }
   ],
   "source": [
    "ex1 = [[\"aaa\", \"bbb\", \"aaa\"],\n",
    "       [\"bbb\", \"aaa\", \"bbb\"],\n",
    "        [\"aaa\", \"bbb\", \"bbb\", \"aaa\"],\n",
    "        [\"uuu\", \"vvv\"],\n",
    "        [\"uuu\", \"vvv\",\"vvv\"],\n",
    "        [\"uuu\", \"vvv\", \"vvv\", \"uuu\"]]\n",
    "\n",
    "\n",
    "test1 = LDA(data=ex1, nr_of_topics=2 , a=0.75, b=0.75, iterations=1000)\n",
    "test1.compileModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "bbb, aaa, \n",
      "Topic 1\n",
      "vvv, uuu, \n"
     ]
    }
   ],
   "source": [
    "test1.mostImportantNWords(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta is\n",
      "\n",
      "[[0.74704154 0.25295846]]\n",
      "[[0.78559115 0.21440885]]\n",
      "[[0.68558918 0.31441082]]\n",
      "[[0.25369908 0.74630092]]\n",
      "[[0.03471068 0.96528932]]\n",
      "[[0.40922949 0.59077051]]\n",
      "Phi is \n",
      "\n",
      "[[0.31595751 0.47970243 0.08612931 0.11821074]]\n",
      "[[0.21522033 0.00903309 0.30815887 0.46758771]]\n",
      "Z is \n",
      "\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[1. 1.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, 1, 0.9979], [0, 2, 0.9953], [1, 0, 0.9979], [2, 0, 0.9953]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1.showResults()\n",
    "test1.documentSimilarity(0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The topic of the document is  1\n"
     ]
    }
   ],
   "source": [
    "test_Text = [\"uuu\", \"uuu\", \"uuuuu\", \"CCC\"]\n",
    "test1.assignNewTopic(test_Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "\n",
    "In order to use a real dataset of text, it needs to be preprocessed in order to remove punctuations, common words and transform the nouns into singular form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = STOPWORDS.union(set(['say']))\n",
    "\n",
    "def lemmatize(text):\n",
    "    return WordNetLemmatizer().lemmatize(text, pos='v')\n",
    "\n",
    "def process_text(text, action=\"lemmatization_and_stemming\"):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in my_stop_words and len(token) > 3:\n",
    "            processed_token = lemmatize(token)\n",
    "            if processed_token not in my_stop_words:\n",
    "                result.append(processed_token)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>politics</td>\n",
       "      <td>howard hits back at mongrel jibe michael howar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>politics</td>\n",
       "      <td>blair prepares to name poll date tony blair is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sport</td>\n",
       "      <td>henman hopes ended in dubai third seed tim hen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sport</td>\n",
       "      <td>wilkinson fit to face edinburgh england captai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>last star wars  not for children  the sixth an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>berlin cheers for anti-nazi film a german movi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>business</td>\n",
       "      <td>virgin blue shares plummet 20% shares in austr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>business</td>\n",
       "      <td>crude oil prices back above $50 cold weather a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>politics</td>\n",
       "      <td>hague  given up  his pm ambition former conser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sport</td>\n",
       "      <td>moya emotional after davis cup win carlos moya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>business</td>\n",
       "      <td>s korean credit card firm rescued south korea ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>politics</td>\n",
       "      <td>howard backs stem cell research michael howard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sport</td>\n",
       "      <td>connors boost for british tennis former world ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>business</td>\n",
       "      <td>japanese banking battle at an end japan s sumi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>tech</td>\n",
       "      <td>games maker fights for survival one of britain...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         category                                               text\n",
       "0            tech  tv future in the hands of viewers with home th...\n",
       "1        business  worldcom boss  left books alone  former worldc...\n",
       "2           sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3           sport  yeading face newcastle in fa cup premiership s...\n",
       "4   entertainment  ocean s twelve raids box office ocean s twelve...\n",
       "5        politics  howard hits back at mongrel jibe michael howar...\n",
       "6        politics  blair prepares to name poll date tony blair is...\n",
       "7           sport  henman hopes ended in dubai third seed tim hen...\n",
       "8           sport  wilkinson fit to face edinburgh england captai...\n",
       "9   entertainment  last star wars  not for children  the sixth an...\n",
       "10  entertainment  berlin cheers for anti-nazi film a german movi...\n",
       "11       business  virgin blue shares plummet 20% shares in austr...\n",
       "12       business  crude oil prices back above $50 cold weather a...\n",
       "13       politics  hague  given up  his pm ambition former conser...\n",
       "14          sport  moya emotional after davis cup win carlos moya...\n",
       "15       business  s korean credit card firm rescued south korea ...\n",
       "16       politics  howard backs stem cell research michael howard...\n",
       "17          sport  connors boost for british tennis former world ...\n",
       "18       business  japanese banking battle at an end japan s sumi...\n",
       "19           tech  games maker fights for survival one of britain..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./bbc-text.csv\")\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the dataset, the BBC News dataset will be used. It contains news text from 5 categories.\n",
    "\n",
    "\n",
    "The dataset is taken from [Kaggle](https://www.kaggle.com/balatmak/newsgroup20bbcnews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-faf7fbdf7fda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mSAMPLES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mLENGTH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "categories = data.category.unique()\n",
    "SAMPLES = 4\n",
    "LENGTH = 30\n",
    "input_data = []\n",
    "# Sample from the big dataset, only the first SAMPLE lines from each category\n",
    "# I'm making sure that the dataset is balanced, equal number of samples from each category\n",
    "for cat in categories:\n",
    "\n",
    "    category_df = data[data.category == cat].head(SAMPLES)\n",
    "    for i, text in enumerate(category_df.text):\n",
    "        np.random.randint(0,i+6)\n",
    "        sentence = process_text(text)\n",
    "        input_data.append(sentence[:LENGTH+i])\n",
    "\n",
    "\n",
    "lda_model = LDA(data=input_data, nr_of_topics=len(categories),iterations=40000, a=0.75,b=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-                 2%                  ] 1082 of 40000 complete in 8.7 secHalting at iteration  1103  of  40000\n"
     ]
    }
   ],
   "source": [
    "lda_model.compileModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta is\n",
      "\n",
      "[[nan nan nan nan nan]]\n",
      "[[nan nan nan nan nan]]\n",
      "[[nan nan nan nan nan]]\n",
      "[[nan nan nan nan nan]]\n",
      "[[nan nan nan nan nan]]\n",
      "[[nan nan nan nan nan]]\n",
      "[[nan nan nan nan nan]]\n",
      "[[nan nan nan nan nan]]\n",
      "[[nan nan nan nan nan]]\n",
      "[[nan nan nan nan nan]]\n",
      "[[nan nan nan nan nan]]\n",
      "[[nan nan nan nan nan]]\n",
      "[[nan nan nan nan nan]]\n",
      "[[nan nan nan nan nan]]\n",
      "[[nan nan nan nan nan]]\n",
      "[[nan nan nan nan nan]]\n",
      "[[nan nan nan nan nan]]\n",
      "[[nan nan nan nan nan]]\n",
      "[[nan nan nan nan nan]]\n",
      "[[nan nan nan nan nan]]\n",
      "Phi is \n",
      "\n",
      "[[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "[[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "[[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "[[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "[[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "Z is \n",
      "\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tudor/.miniconda3/envs/pp/lib/python3.6/site-packages/ipykernel_launcher.py:65: RuntimeWarning: Mean of empty slice.\n",
      "/home/tudor/.miniconda3/envs/pp/lib/python3.6/site-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "/home/tudor/.miniconda3/envs/pp/lib/python3.6/site-packages/ipykernel_launcher.py:66: RuntimeWarning: Mean of empty slice.\n",
      "/home/tudor/.miniconda3/envs/pp/lib/python3.6/site-packages/ipykernel_launcher.py:67: RuntimeWarning: Mean of empty slice.\n"
     ]
    }
   ],
   "source": [
    "lda_model.showResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tudor/.miniconda3/envs/pp/lib/python3.6/site-packages/ipykernel_launcher.py:94: RuntimeWarning: Mean of empty slice.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.documentSimilarity(threshold= 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "alzheimer, victims, december, crude, price, cold, weather, unite, state, europe, \n",
      "Topic 1\n",
      "alzheimer, victims, december, crude, price, cold, weather, unite, state, europe, \n",
      "Topic 2\n",
      "alzheimer, victims, december, crude, price, cold, weather, unite, state, europe, \n",
      "Topic 3\n",
      "alzheimer, victims, december, crude, price, cold, weather, unite, state, europe, \n",
      "Topic 4\n",
      "alzheimer, victims, december, crude, price, cold, weather, unite, state, europe, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tudor/.miniconda3/envs/pp/lib/python3.6/site-packages/ipykernel_launcher.py:82: RuntimeWarning: Mean of empty slice.\n"
     ]
    }
   ],
   "source": [
    "lda_model.mostImportantNWords(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['clarke', 'press', 'card', 'home', 'secretary', 'charles', 'clarke', 'vow', 'plough', 'plan', 'card', 'despite', 'pause', 'think', 'charles', 'kennedy', 'leader', 'david', 'blunkett', 'resignation', 'good', 'opportunity', 'question', 'legislation', 'necessary', 'clarke', 'support', 'plan', 'blunkett', 'argue', 'cabinet', 'support', 'card', 'mean', 'create', 'secure', 'society', 'clarke', 'acknowledge', 'measure', 'introduce', 'remain', 'matter', 'debate', 'legislation', 'significantly', 'influence', 'recommendations', 'commons', 'home', 'affairs', 'committee', 'issue', 'debate', 'parliament', 'monday', 'schedule', 'insist', 'earlier', 'kennedy', 'party', 'oppose', 'card', 'plan', 'deeply', 'flaw', 'christmas', 'come', 'home', 'secretary', 'time', 'think', 'tell', 'radio', 'today', 'programme', 'clarke', 'report', 'enthusiastic', 'card', 'predecessors', 'wouldn', 'good', 'opportunity', 'home', 'secretary', 'broom', 'sweep', 'clean', 'respect', 'need', 'legislation', 'place', 'ask', 'earlier', 'week', 'tories', 'announce', 'government', 'plan', 'michael', 'howard', 'force', 'deny', 'shadow', 'cabinet', 'split', 'decision', 'decide', 'support', 'plan', 'police', 'help', 'fight', 'terror', 'crime', 'illegal', 'immigration', 'report', 'reservations', 'strategy', 'senior', 'shadow', 'cabinet', 'members', 'david', 'davis', 'oliver', 'letwin', 'chairman', 'council', 'mansfield', 'warn', 'real', 'risk', 'people', 'margins', 'society', 'drive', 'hand', 'extremists', 'happen', 'young', 'asian', 'bomb', 'stop', 'haven', 'card', 'detain', 'home', 'office', 'people', 'passport', 'card', 'undecided', 'separate', 'card', 'card', 'issue', 'introduce', 'blunkett', 'suggest', 'parliament', 'decide', 'compulsory', 'everybody', 'card', 'carry', 'create', 'criminal', 'offences', 'possession', 'false', 'identity', 'document', 'civil', 'penalties', 'include', 'fine', 'fine', 'people', 'fail', 'house', 'change', 'fail', 'sign', 'card', 'compulsory', 'scheme', 'oversee', 'independent', 'watchdog']\n"
     ]
    }
   ],
   "source": [
    "# Generate a random text from the database\n",
    "categories = data.category.unique()\n",
    "random_col = np.random.randint(len(data.index))\n",
    "test_Text = process_text(data.iloc[random_col][1])\n",
    "real_category = data.iloc[random_col][0]\n",
    "print(test_Text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The topic of the document is  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tudor/.miniconda3/envs/pp/lib/python3.6/site-packages/ipykernel_launcher.py:117: RuntimeWarning: Mean of empty slice.\n"
     ]
    }
   ],
   "source": [
    "lda_model.assignNewTopic(test_Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 \n",
    "\n",
    "\n",
    "### Topic based similarity\n",
    "\n",
    "The similarity is calculated using the [helinger distance](https://en.wikipedia.org/wiki/Hellinger_distance) among each document. In this case I have managed to compare all pairs of documents and if the similiarity is above a certain treshold, it is printed.\n",
    "\n",
    "### Assigning new topics\n",
    "\n",
    "In order to assign a new topic to a document, the document is split into individual words, then each word is encoded (previously unseen words are skipped). Then for each topic we calculate the sum of probabilities for each word. To get the topic we return the index of the highest sum of probabilities. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pp] *",
   "language": "python",
   "name": "conda-env-pp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
